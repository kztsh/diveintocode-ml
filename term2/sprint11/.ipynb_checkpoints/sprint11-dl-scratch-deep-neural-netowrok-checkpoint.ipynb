{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term1 Sprint11 授業課題 \n",
    "## コーディング課題：ディープニューラルネットワーク(DNN)スクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していく。  \n",
    "前回作成した3層のニューラルネットワークを、クラスを活用することで、任意の構成に拡張しやすいコードに書き換えていく。  \n",
    "その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていく。  \n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指す。  \n",
    "\n",
    "**新たなニューラルネットワーク分類器のクラスを作成する**  \n",
    "Sprint10で作成したものとは別に、ニューラルネットワーク分類器のクラスScratchDeepNeuralNetrowkClassifierを作成する。\n",
    "\n",
    "**層などのクラス化**  \n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていく。  \n",
    "\n",
    "-手を加える箇所-  \n",
    "- 層の数\n",
    "- 層の種類（今後は畳み込み層など他のタイプの層が登場する）\n",
    "- 活性化関数の種類\n",
    "- 重みやバイアスの初期化方法\n",
    "- 最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 全結合層のクラス化\n",
    "**コーディング**\n",
    "\n",
    "コンストラクタで重みやバイアスの初期化をし、フォワードとバックワードのメソッドを用意する。  \n",
    "重み$W$、バイアス$B$、およびフォワード時の入力$X$をインスタンス変数として保持しておくことで、煩雑な入出力は不要になる。  \n",
    "\n",
    "なお、インスタンスも引数として渡すことができる。  \n",
    "そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われる。  \n",
    "渡すインスタンスを変えれば、初期化方法が変えられる。\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできる。  \n",
    "これを利用してself = self.optimizer.update(self)として層の重みの更新が可能。  \n",
    "更新に必要な値は複数あるが、全て全結合層が持つインスタンス変数にすることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyprind\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (60000, 28, 28)\n",
      "X_test.shape:  (10000, 28, 28)\n",
      "X_train[0].dtype:  uint8\n"
     ]
    }
   ],
   "source": [
    "# データ生成\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"X_train.shape: \", X_train.shape) # (60000, 28, 28)\n",
    "print(\"X_test.shape: \", X_test.shape) # (10000, 28, 28)\n",
    "print(\"X_train[0].dtype: \", X_train[0].dtype) # uint8\n",
    "\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (48000, 784)\n",
      "X_val.shape:  (12000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 訓練データから更に検証データを生成\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(\"X_train.shape: \", X_train.shape) # (48000, 784)\n",
    "print(\"X_val.shape: \", X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetworkClassifier:\n",
    "    \"\"\"\n",
    "    3層(入力層も含めると4層)ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int (default : 400)\n",
    "        隠れ層1のノード数\n",
    "    n_nodes2 : int (default : 200)\n",
    "        隠れ層2のノード数\n",
    "    n_epochs : int (default : 10)\n",
    "        エポック回数\n",
    "    learning_method : str (default : \"SGD\")\n",
    "        学習の際の最適化手法(\"SGD\", \"AdaGrad\"から選択)\n",
    "    alpha : float (default : 0.01)\n",
    "        学習率\n",
    "    batch_size : int (default : 10)\n",
    "        ミニバッチサイズ\n",
    "    sigma : str or float (default : 0.01)\n",
    "        各層の重み係数の初期値の標準偏差(\"Xavier\", \"He\"から選択または値を直接入力)\n",
    "    activation : str (default : \"tanh\")\n",
    "        活性化関数 (\"sigmoid\", \"tanh\", ReLU\"のいずれかを選択)\n",
    "    random_seed : int (default : None)\n",
    "        各層の重み係数の初期化および各エポック毎のデータシャッフル用の擬似乱数シード\n",
    "    verbose : bool (default : True)\n",
    "        学習過程を出力しない場合はFalseを設定\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.train_cost_ : list\n",
    "        1エポック毎の訓練データのコスト関数値（交差エントロピー誤差）\n",
    "    self.val_cost_ : list\n",
    "        1エポック毎の検証データのコスト関数値（交差エントロピー誤差）\n",
    "    self.train_accuracy_ : list\n",
    "        1エポック毎の訓練データの正解率\n",
    "    self.val_accuracy_ : list\n",
    "        1エポック毎の検証データの正解率\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1=400, n_nodes2=200, n_epochs=10, \n",
    "                 learning_method=\"SGD\", alpha=0.01, batch_size=10, \n",
    "                 sigma=0.01, activation=\"tanh\", random_seed=None, verbose=True):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_method = learning_method\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.sigma = sigma\n",
    "        self.activation = activation\n",
    "        self.random_seed = random_seed # バッチ学習用\n",
    "        self.random = np.random.RandomState(random_seed) # 重み初期化、各epoch毎のデータシャッフル用\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y_train : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        # 出力層のノード数、データの特徴量数\n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "        # コスト関数値、正解率の空リストを生成\n",
    "        self.train_cost_ = []\n",
    "        self.val_cost_ = []\n",
    "        self.train_accuracy_ = []\n",
    "        self.val_accuracy_ = []\n",
    "        \n",
    "        # 全結合層のインスタンスを作成\n",
    "        optimizer = LearningMethod(self.learning_method, self.alpha)\n",
    "        self.layer1 = FullConnLayer(n_features, self.n_nodes1, \n",
    "                                    SimpleInitializer(self.sigma, n_features), optimizer)\n",
    "        self.layer2 = FullConnLayer(self.n_nodes1, self.n_nodes2, \n",
    "                                    SimpleInitializer(self.sigma, self.n_nodes1), optimizer)\n",
    "        self.layer_out = FullConnLayer(self.n_nodes2, n_output, \n",
    "                                       SimpleInitializer(self.sigma, self.n_nodes2), optimizer)\n",
    "        # 活性化関数のインスタンスを作成\n",
    "        self.activator1 = Activator(self.activation)\n",
    "        self.activator2 = Activator(self.activation)\n",
    "        self.activator_out = Softmax()\n",
    "        \n",
    "        # yラベルをOneHotEncoding\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_onehot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "        y_val_onehot = enc.transform(y_val[:, np.newaxis])\n",
    "        \n",
    "        # 学習の進捗バーを設定\n",
    "        pbar = pyprind.ProgBar(self.n_epochs * (np.ceil(X_train.shape[0] / self.batch_size).astype(np.int)))\n",
    "        \n",
    "        # エポック毎に学習を繰り返す\n",
    "        for i in range(self.n_epochs):\n",
    "            get_mini_batch = GetMiniBatch(X_train, y_train_onehot, \n",
    "                                          batch_size=self.batch_size, seed=self.random_seed)\n",
    "            \n",
    "            # 1エポック毎にバッチデータ生成して学習\n",
    "            for X_train_batch, y_train_batch in get_mini_batch:\n",
    "                \n",
    "                # Forward propagation\n",
    "                z_n1, a_n1, z_n2, a_n2, z_out, a_out = self._forward(X_train_batch)\n",
    "                \n",
    "                # Backward propagation\n",
    "                # grad_z_n3は交差エントロピー誤差とソフトマックスを合わせている\n",
    "                grad_z_out = self.activator_out.backward(a_out, y_train_batch)\n",
    "                grad_a_n2 = self.layer_out.backward(grad_z_out)\n",
    "                grad_z_n2 = self.activator2.backward(grad_a_n2)\n",
    "                grad_a_n1 = self.layer2.backward(grad_z_n2)\n",
    "                grad_z_n1 = self.activator1.backward(grad_a_n1)\n",
    "                # grad_a_n0は使用しないが、重み更新のためbackward実行\n",
    "                grad_a_n0 = self.layer1.backward(grad_z_n1)\n",
    "                \n",
    "                # 進捗バーを更新\n",
    "                pbar.update()\n",
    "            \n",
    "            # 1エポック毎に訓練データのコスト関数値、正解率を記録\n",
    "            z_n1, a_n1, z_n2, a_n2, z_out, a_out = self._forward(X_train)\n",
    "            # コスト関数値\n",
    "            value1 = y_train_onehot * (np.log(a_out + 1e-05)) # ゼロ除算対策\n",
    "            value2 = (1. - y_train_onehot) * np.log(1. - a_out + 1e-05) # ゼロ除算対策\n",
    "            train_cost = -np.sum(value1 + value2)\n",
    "            self.train_cost_.append(train_cost)\n",
    "            # 正解率\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            train_accuracy = (np.sum(y_train == y_train_pred).astype(np.float)) / y_train.shape[0]\n",
    "            self.train_accuracy_.append(train_accuracy)\n",
    "            \n",
    "            # 検証データがあれば同様に記録\n",
    "            if X_val is not None and y_val is not None:\n",
    "                z_n1, a_n1, z_n2, a_n2, z_out, a_out = self._forward(X_val)\n",
    "                # コスト関数値\n",
    "                value1 = y_val_onehot * (np.log(a_out + 1e-05)) # ゼロ除算対策\n",
    "                value2 = (1. - y_val_onehot) * np.log(1. - a_out + 1e-05) # ゼロ除算対策\n",
    "                val_cost = -np.sum(value1 + value2)\n",
    "                self.val_cost_.append(val_cost)\n",
    "                # 正解率\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_accuracy = (np.sum(y_val == y_val_pred).astype(np.float)) / y_val.shape[0]\n",
    "                self.val_accuracy_.append(val_accuracy)\n",
    "        \n",
    "        #verboseをTrueにした際は学習過程などを出力(コスト関数値の表示は学習開始時点で正規化)\n",
    "        if self.verbose:\n",
    "            plt.rcParams[\"font.size\"] = 16\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(20,9))\n",
    "            ax[0].plot(np.arange(1, len(self.train_cost_)+1), self.train_cost_ / self.train_cost_[0], \n",
    "                       linewidth=2, linestyle=\"-\", marker=\"o\", color=\"steelblue\", label=\"Train\")\n",
    "            ax[0].set_xlabel(\"epoch numbers\")\n",
    "            ax[0].set_ylabel(\"Normalized cost value\")\n",
    "            ax[1].plot(np.arange(1, len(self.train_accuracy_)+1), self.train_accuracy_, \n",
    "                       linewidth=2, linestyle=\"-\", marker=\"o\", color=\"steelblue\", label=\"Train\")\n",
    "            ax[1].set_xlabel(\"epoch numbers\")\n",
    "            ax[1].set_ylabel(\"accuracy\")\n",
    "            # 検証データも学習したら可視化(コスト関数値の表示は学習開始時点で正規化)\n",
    "            if len(self.val_cost_) > 0:\n",
    "                ax[0].plot(np.arange(1, len(self.val_cost_)+1), self.val_cost_ / self.val_cost_[0], \n",
    "                           linewidth=2, linestyle=\"--\", marker=\"o\", color=\"orangered\", label=\"Validation\")\n",
    "                ax[1].plot(np.arange(1, len(self.val_accuracy_)+1), self.val_accuracy_, \n",
    "                           linewidth=2, linestyle=\"--\", marker=\"o\", color=\"orangered\", label=\"Validation\")\n",
    "            ax[0].legend()\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : 次の形のndarray, shape (n_samples, n_features)\n",
    "            テスト用データの特徴量\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : 次の形のndarray, shape (n_samples, 1)\n",
    "            予測ラベル\n",
    "        \"\"\"\n",
    "        z_n1, a_n1, z_n2, a_n2, z_out, a_out = self._forward(X_test)\n",
    "        return np.argmax(z_out, axis=1) # 出力層への入力値で最大のノード位置を出力\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        # Forward propagation\n",
    "        z_n1 = self.layer1.forward(X)\n",
    "        a_n1 = self.activator1.forward(z_n1)\n",
    "        z_n2 = self.layer2.forward(a_n1)\n",
    "        a_n2 = self.activator2.forward(z_n2)\n",
    "        z_out = self.layer_out.forward(a_n2)\n",
    "        a_out = self.activator_out.forward(z_out)\n",
    "        return z_n1, a_n1, z_n2, a_n2, z_out, a_out\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma, n):\n",
    "        if (type(sigma) is float) or (type(sigma) is int):\n",
    "            self.sigma = float(sigma)\n",
    "        elif sigma == \"Xavier\":\n",
    "            self.sigma = np.sqrt(1. / n)\n",
    "        elif sigma == \"He\":\n",
    "            self.sigma = np.sqrt(2. / n)\n",
    "        else:\n",
    "            raise ValueError(\"Set \\\"Xavier\\\" or \\\"He\\\" or numerical value (float or int).\")\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        return self.sigma * np.random.randn(n_nodes2)\n",
    "\n",
    "class LearningMethod:\n",
    "    def __init__(self, learning_method, alpha):\n",
    "        self.learning_method = learning_method\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def update(self, layer):\n",
    "        if self.learning_method == \"SGD\":\n",
    "            layer.w -= self.alpha * layer.grad_w / layer.grad_b.shape[0]\n",
    "            layer.b -= self.alpha * layer.grad_b.mean(axis=0)\n",
    "            return layer\n",
    "        elif self.learning_method == \"AdaGrad\":\n",
    "            layer.h_w += (layer.grad_w)**2\n",
    "            layer.h_b += (layer.grad_b)**2\n",
    "            layer.w -= self.alpha * (np.sqrt(1. / layer.h_w)) * layer.grad_w / layer.grad_b.shape[0]\n",
    "            layer.b -= self.alpha * ((np.sqrt(1. / layer.h_b)) * layer.grad_b).mean(axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Set \\\"SGD\\\" or \\\"AdaGrad\\\".\")\n",
    "\n",
    "class FullConnLayer:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # initializerのメソッドを使い、self.wとself.bを初期化する\n",
    "        self.w = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer.learning_method == \"AdaGrad\":\n",
    "            self.h_w = np.ones((n_nodes1, n_nodes2))\n",
    "            self.h_b = np.ones(n_nodes2)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力データ\n",
    "        Returns\n",
    "        ----------\n",
    "        a : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力データ\n",
    "        \"\"\"\n",
    "        # backward用に入力値zを保存しておく\n",
    "        self.z = z\n",
    "        a = np.dot(z, self.w) + self.b # shape: (n_samples, n_nodes1)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, grad_a):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad_a : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        grad_z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.grad_w = np.dot(self.z.T, grad_a) # shape: (n_nodes1, n_nodes2)\n",
    "        self.grad_b = grad_a.sum(axis=0) # shape: (n_nodes2,)\n",
    "        grad_z = np.dot(grad_a, self.w.T) # shape: (n_samples, n_nodes2)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return grad_z\n",
    "\n",
    "class Activator:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, a):\n",
    "        # backward用に入力値aを保存しておく\n",
    "        self.a = a\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-a))\n",
    "        elif self.activation == \"tanh\":\n",
    "            return np.tanh(a)\n",
    "        elif self.activation == \"ReLU\":\n",
    "            return np.maximum(0, a)\n",
    "        else:\n",
    "            raise ValueError(\"Set \\\"sigmoid\\\" or \\\"tanh\\\" or \\\"ReLU\\\".\")\n",
    "            \n",
    "    def backward(self, grad_a):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return grad_a * (self.a * (1. - self.a))\n",
    "        elif self.activation == \"tanh\":\n",
    "            return grad_a * (1. - (np.tanh(self.a))**2)\n",
    "        elif self.activation == \"ReLU\":\n",
    "            return np.where(self.a >= 0, self.a, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Set \\\"sigmoid\\\" or \\\"tanh\\\" or \\\"ReLU\\\".\")\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, a):\n",
    "        a_max = np.max(a, axis=1)\n",
    "        exp_a = np.exp(a - a_max.reshape(-1, 1)) # a_maxはオーバーフロー対策\n",
    "        sum_exp_a = np.sum(exp_a, axis=1).reshape(-1, 1)\n",
    "        return exp_a / sum_exp_a\n",
    "    \n",
    "    def backward(self, a_out, y):\n",
    "        return a_out - y\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    このクラスをニューラルネットワークのクラス内でインスタンス化し、for文を使うことでミニバッチを取り出す\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    # 1エポック内でのイテレーション回数を返す\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    # 各イテレーションで使うバッチデータを返す\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = item * self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    # イテレーションのカウント数をゼロにする\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    # 次のイテレーションで使うバッチデータを返す\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter * self.batch_size\n",
    "        p1 = self._counter * self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のコードを実際に実行してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##                            ] 100% | ETA: 00:00:54"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-10af7eaf8823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     batch_size=10, sigma=0.1, activation=\"tanh\", random_seed=0, verbose=True)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscratch_snnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-5848855a34d5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mgrad_z_n1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivator1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_a_n1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# grad_a_n0は使用しないが、重み更新のためbackward実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mgrad_a_n0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_z_n1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# 進捗バーを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-5848855a34d5>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_a)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mgrad_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape: (n_samples, n_nodes2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# 更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-5848855a34d5>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"AdaGrad\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_w\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_b\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# layer.w -= self.alpha * (np.sqrt(1. / layer.h_w)) * layer.grad_w / layer.grad_b.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 「SGDとAdagradの選択」、「層の深さを選択」、「sigmoidのオーバーフロー対策」を新たに設計\n",
    "# 実際に実行\n",
    "scratch_snnc = ScratchDeepNeuralNetworkClassifier(\n",
    "    n_nodes1=400, n_nodes2=200, n_epochs=2, learning_method=\"AdaGrad\", alpha=2e-3, \n",
    "    batch_size=10, sigma=0.1, activation=\"tanh\", random_seed=0, verbose=True)\n",
    "\n",
    "scratch_snnc.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終学習エポックの正解率を出力\n",
    "final_train_accuracy = scratch_snnc.train_accuracy_[-1].round(4)\n",
    "final_val_accuracy = scratch_snnc.val_accuracy_[-1].round(4)\n",
    "\n",
    "# 最終学習エポックの推測ラベルを出力\n",
    "y_train_pred = scratch_snnc.predict(X_train)\n",
    "y_val_pred = scratch_snnc.predict(X_val)\n",
    "\n",
    "# 正解数を算出\n",
    "train_accuracy_num = (y_train == y_train_pred).sum()\n",
    "val_accuracy_num = (y_val == y_val_pred).sum()\n",
    "\n",
    "# 正解率を出力\n",
    "print(\"Final train accuracy: {} ({}/{})\".format(final_train_accuracy, train_accuracy_num, len(y_train)))\n",
    "print(\"Final validation accuracy: {} ({}/{})\".format(final_val_accuracy, val_accuracy_num, len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 初期化方法のクラス化\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにする。  \n",
    "雛形に必要なコードを書き加えていく。  \n",
    "標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになる。\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "    \"\"\"\n",
    "    重みの初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    W :\n",
    "    \"\"\"\n",
    "        pass\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "    \"\"\"\n",
    "    バイアスの初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    B :\n",
    "    \"\"\"\n",
    "        pass\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 最適化手法のクラス化\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡す。  \n",
    "バックワードのときにself = self.optimizer.update(self)のように更新できるようにする。\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 活性化関数のクラス化\n",
    "活性化関数もクラス化を行う。\n",
    "\n",
    "class SGDではソフトマックス関数のバックプロパゲーションに交差エントロピー誤差の計算も含む実装を想定している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装する。  \n",
    "\n",
    "$$% <![CDATA[\n",
    "f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>$$\n",
    "\n",
    "$x$ : ある特徴量。スカラー\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能。\n",
    "\n",
    "[numpy.maximum — NumPy v1.15 Manual](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html \"numpy.maximum — NumPy v1.15 Manual\")\n",
    "\n",
    "一方、バックプロパゲーションのための$x$に関する$f(x)$の微分は以下のようになる。\n",
    "\n",
    "$$% <![CDATA[\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>$$\n",
    "\n",
    "数学的には微分可能ではないが、$x=0$のとき$0$とすることで対応している。  \n",
    "フォワード時の$x$の正負により、勾配を逆伝播するかどうかが決まるということになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきた。  \n",
    "しかし、一般的に良い初期値の取り方が知られている。\n",
    "\n",
    "シグモイド関数やハイパボリックタンジェント関数のときはXavierの初期値（またはGlorotの初期値）、ReLUのときはHeの初期値が使われる。\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成する。\n",
    "\n",
    "それぞれの初期化方法における$\\sigma$は次の式で求められる。\n",
    "\n",
    "「Xavierの初期値」\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}}$$\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "（論文）\n",
    "\n",
    "[Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf \"Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.\")\n",
    "\n",
    "「Heの初期値」\n",
    "$$\\sigma = \\sqrt{\\frac{2}{n}}$$\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "（論文）\n",
    "\n",
    "[He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.](https://arxiv.org/pdf/1502.01852.pdf \"He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 最適化手法\n",
    "**コーディング**\n",
    "\n",
    "学習率は学習の良し悪しにとって重要なハイパーパラメータであり、これを学習過程で変化させていく方法が現在では一般的。  \n",
    "様々な手法が提案されているが、今回はその中でも基本的な、AdaGradを実装する。\n",
    "\n",
    "まず、これまで使ってきたSGDを確認する。\n",
    "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) $$\n",
    "\n",
    "$$B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
    "\n",
    "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）  \n",
    "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$に関する損失$L$の勾配\n",
    "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$に関する損失$L$の勾配\n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "続いて、AdaGradではバイアスの数式は省略するが、重みと同様のことをする。\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていく。\n",
    "\n",
    "イテレーションごとの勾配の二乗和$H$を保存しておき、その分だけ学習率を小さくする。\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになる。\n",
    "$$H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})$$\n",
    "\n",
    "$$W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) $$\n",
    "\n",
    "$H_i$ : $i$層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "$H′_i$ : 更新した$H_i$\n",
    "\n",
    "AdaGradクラスを作成し、上記の数式にもとづいて実装する。\n",
    "\n",
    "（論文）\n",
    "\n",
    "[Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf \"Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
