{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term1 Sprint16 授業課題 \n",
    "## コーディング課題：論文読解入門 (Faster R-CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**論文読解**  \n",
    "以下の論文を読み問題に答える。CNNを使った物体検出（Object Detection）の代表的な研究である。\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n",
    "**問題**  \n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述する。\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "(8) （アドバンス）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    "\n",
    "**条件**  \n",
    "答える際は論文のどの部分からそれが分かるかを書く。\n",
    "必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "- Selective search:  \n",
    "1.画像を小さい領域群に分割  \n",
    "2.隣接領域同士の類似度を計算  \n",
    "3.最も類似度の高い領域を統合  \n",
    "4.2~3を繰り返す  \n",
    "\n",
    "- SPPnet:  \n",
    "画像の切り取り(crop)や歪ませ(warp)の後に畳み込み層と全結合層（固定サイズの入力が必要）へ入力させるのではなく、画像を最初に畳み込み層へ入力させ、その任意のサイズの特徴マップをspatial pyramid poolingにより既定サイズの出力とすることで、精度と処理速度を向上させる\n",
    "\n",
    "- R-CNN:  \n",
    "物体クラス認識用の大規模データセットを利用して、畳み込み層をあらかじめ学習させておく。Selective searchを用いて複数のバウンディングボックスを提案し、畳み込み層へ入力する。出力された特徴量マップを分類器（線形SVM）へ入力して分類する。この際、non-maximal-supressionを行って不要なバウンディングボックスを排除し、より良い精度を求めてバウンディングボックスのサイズをパラメータとして回帰学習させる。\n",
    "\n",
    "- Fast R-CNN:  \n",
    "大量に提案されるバウンディングボックスを逐次計算処理する必要があるR-CNNの改良版。入力画像を畳み込み層へ入力し、出力される特徴量マップにてバウンディングボックスを提案することで、畳み込み計算を提案毎に逐一計算する必要がない。プーリング層(spatial pyramid pooling)にて既定サイズに変換し、全結合層へ入力後はR-CNNと同様。クラス認識の損失とバウンディングボックス回帰の損失を合わせたマルチタスク損失式を定義して計算の効率化も行っている。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**1.Introduction**  \n",
    "Recent advances in object detection are driven by\n",
    "the success of region proposal methods (e.g., [4])\n",
    "and region-based convolutional neural networks (RCNNs) [5]. Although region-based CNNs were computationally expensive as originally developed in [5],\n",
    "their cost has been drastically reduced thanks to sharing convolutions across proposals [1], [2]. The latest\n",
    "incarnation, Fast R-CNN [2], achieves near real-time\n",
    "rates using very deep networks [3], when ignoring the\n",
    "time spent on region proposals. Now, proposals are the\n",
    "test-time computational bottleneck in state-of-the-art\n",
    "detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。  \n",
    "従来のSelective searchではなく、畳み込み層の特徴量マップからバウンディングボックスを提案するネットワーク(Region Proposal Network, RPN)を構築する。Region ProposalもDNN化してend-to-endで予測を行う統合ネットワークとすることで高速化を実現している。既定の形と位置のアンカーボックスと実際に提案されたバウンディングボックスとの差（位置とアスペクト比に関する損失）、物体の有無（分類）に関する損失について、Fast R-CNNと同様にマルチタスク損失式を用いて最適化する。バウンディングボックスの一致の度合いの判定にはIoU(Intersection over Union)を用い、複数のアンカーの中でground-truth box（教師データ）とのIoUが最高または0.7以上のアンカーを「物体あり」のラベルとして分類する。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**3.1 Region Proposal Networks**  \n",
    "A Region Proposal Network (RPN) takes an image\n",
    "(of any size) as input and outputs a set of rectangular\n",
    "object proposals, each with an objectness score.3 We\n",
    "model this process with a fully convolutional network\n",
    "[7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN\n",
    "object detection network [2], we assume that both nets\n",
    "share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model\n",
    "[32] (ZF), which has 5 shareable convolutional layers\n",
    "and the Simonyan and Zisserman model [3] (VGG-16),\n",
    "which has 13 shareable convolutional layers.\n",
    "To generate region proposals, we slide a small\n",
    "network over the convolutional feature map output\n",
    "by the last shared convolutional layer. This small\n",
    "network takes as input an n × n spatial window of\n",
    "the input convolutional feature map. Each sliding\n",
    "window is mapped to a lower-dimensional feature\n",
    "(256-d for ZF and 512-d for VGG, with ReLU [33]\n",
    "following). This feature is fed into two sibling fullyconnected layers—a box-regression layer (reg) and a\n",
    "box-classification layer (cls). We use n = 3 in this\n",
    "paper, noting that the effective receptive field on the\n",
    "input image is large (171 and 228 pixels for ZF and\n",
    "VGG, respectively). This mini-network is illustrated\n",
    "at a single position in Figure 3 (left). Note that because the mini-network operates in a sliding-window\n",
    "fashion, the fully-connected layers are shared across\n",
    "all spatial locations. This architecture is naturally implemented with an n×n convolutional layer followed\n",
    "by two sibling 1 × 1 convolutional layers (for reg and\n",
    "cls, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。  \n",
    "Two stageの手法では、従来のFast R-CNN検出器がアンカーに基づいて領域の提案を絞っていく第一ステージに加え、第二ステージでプーリングを行う。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**4 EXPERIMENTS**  \n",
    "**4.1 Experiments on PASCAL VOC**  \n",
    "One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection\n",
    "method that uses regressors and classifiers on sliding\n",
    "windows over convolutional feature maps. OverFeat\n",
    "is a one-stage, class-specific detection pipeline, and ours\n",
    "is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the\n",
    "region-wise features come from a sliding window of\n",
    "one aspect ratio over a scale pyramid. These features\n",
    "are used to simultaneously determine the location and\n",
    "category of objects. In RPN, the features are from\n",
    "square (3×3) sliding windows and predict proposals\n",
    "relative to anchors with different scales and aspect\n",
    "ratios. Though both methods use sliding windows, the\n",
    "region proposal task is only the first stage of Faster RCNN—the downstream Fast R-CNN detector attends\n",
    "to the proposals to refine them. In the second stage of\n",
    "our cascade, the region-wise features are adaptively\n",
    "pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe\n",
    "these features lead to more accurate detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) RPNとは何か。  \n",
    "畳み込み層の特徴量マップからバウンディングボックスを提案するネットワークのこと。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**3.1 Region Proposal Networks**  \n",
    "A Region Proposal Network (RPN) takes an image\n",
    "(of any size) as input and outputs a set of rectangular\n",
    "object proposals, each with an objectness score.3 We\n",
    "model this process with a fully convolutional network\n",
    "[7], which we describe in this section. Because our ultimate goal is to share computation with a Fast R-CNN\n",
    "object detection network [2], we assume that both nets\n",
    "share a common set of convolutional layers. In our experiments, we investigate the Zeiler and Fergus model\n",
    "[32] (ZF), which has 5 shareable convolutional layers\n",
    "and the Simonyan and Zisserman model [3] (VGG-16),\n",
    "which has 13 shareable convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) RoIプーリングとは何か。  \n",
    "畳み込み層からの特徴量マップにおいて、RPNの提案領域(Region of Interest, RoI)においてプーリングを行う層のこと。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**3 FASTER R-CNN**  \n",
    "**3.2 Sharing Features for RPN and Fast R-CNN**  \n",
    "(iii) Non-approximate joint training. As discussed\n",
    "above, the bounding boxes predicted by RPN are\n",
    "also functions of the input. The RoI pooling layer\n",
    "[2] in Fast R-CNN accepts the convolutional features\n",
    "and also the predicted bounding boxes as input, so\n",
    "a theoretically valid backpropagation solver should\n",
    "also involve gradients w.r.t. the box coordinates. These\n",
    "gradients are ignored in the above approximate joint\n",
    "training. In a non-approximate joint training solution,\n",
    "we need an RoI pooling layer that is differentiable\n",
    "w.r.t. the box coordinates. This is a nontrivial problem\n",
    "and a solution can be given by an “RoI warping” layer\n",
    "as developed in [15], which is beyond the scope of this\n",
    "paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Anchorのサイズはどうするのが適切か。  \n",
    "使用するデータセットの対象物の大きさによって設定する。Microsoft COCO object detection datasetでは、3種のアスペクト比(1:1, 1:2, 2:1)と4種のスケール($64^2, 128^2, 256^2, 512^2$)を使用している。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**3 FASTER R-CNN**  \n",
    "**3.3 Implementation Details**  \n",
    "The anchor boxes that cross image boundaries need\n",
    "to be handled with care. During training, we ignore\n",
    "all cross-boundary anchors so they do not contribute\n",
    "to the loss. For a typical 1000 × 600 image, there\n",
    "will be roughly 20000 (≈ 60 × 40 × 9) anchors in\n",
    "total. With the cross-boundary anchors ignored, there\n",
    "are about 6000 anchors per image for training. If the\n",
    "boundary-crossing outliers are not ignored in training,\n",
    "they introduce large, difficult to correct error terms in\n",
    "the objective, and training does not converge. During\n",
    "testing, however, we still apply the fully convolutional\n",
    "RPN to the entire image. This may generate crossboundary proposal boxes, which we clip to the image\n",
    "boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。  \n",
    "PASCAL VOC 2007, 2012とMS COCO datasetsを使い、いずれも従来のSelective SearchのFast R-CNNに比べ、Faster R-CNNとVGG-16モデルを用いた場合の方が、mAP(mean Average Precision)やフレームレート(fps)が大幅に改善している。さらに、両方のデータをミックスしてより大規模なデータで学習・評価する方がさらに良い結果を生む。\n",
    "\n",
    "------論文該当箇所------  \n",
    "**4 EXPERIMENTS**  \n",
    "**4.3 From MS COCO to PASCAL VOC**  \n",
    "Large-scale data is of crucial importance for improving deep neural networks. Next, we investigate how\n",
    "the MS COCO dataset can help with the detection\n",
    "performance on PASCAL VOC.\n",
    "As a simple baseline, we directly evaluate the\n",
    "COCO detection model on the PASCAL VOC dataset,\n",
    "without fine-tuning on any PASCAL VOC data. This\n",
    "evaluation is possible because the categories on\n",
    "COCO are a superset of those on PASCAL VOC. The\n",
    "categories that are exclusive on COCO are ignored in\n",
    "this experiment, and the softmax layer is performed\n",
    "only on the 20 categories plus background. The mAP\n",
    "under this setting is 76.1% on the PASCAL VOC 2007\n",
    "test set (Table 12). This result is better than that trained\n",
    "on VOC07+12 (73.2%) by a good margin, even though\n",
    "the PASCAL VOC data are not exploited.\n",
    "Then we fine-tune the COCO detection model on\n",
    "the VOC dataset. In this experiment, the COCO model\n",
    "is in place of the ImageNet-pre-trained model (that\n",
    "is used to initialize the network weights), and the\n",
    "Faster R-CNN system is fine-tuned as described in\n",
    "Section 3.2. Doing so leads to 78.8% mAP on the\n",
    "PASCAL VOC 2007 test set. The extra data from\n",
    "the COCO set increases the mAP by 5.6%. Table 6\n",
    "shows that the model trained on COCO+VOC has\n",
    "the best AP for every individual category on PASCAL\n",
    "VOC 2007. Similar improvements are observed on the\n",
    "PASCAL VOC 2012 test set (Table 12 and Table 7). We\n",
    "note that the test-time speed of obtaining these strong\n",
    "results is still about 200ms per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
