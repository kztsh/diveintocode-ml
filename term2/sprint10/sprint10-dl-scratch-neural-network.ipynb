{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term1 Sprint10 授業課題 \n",
    "## コーディング課題：ニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していく。  \n",
    "今回は多クラス分類を行う3層のニューラルネットワークを作成する。  \n",
    "層の数などは固定した上でニューラルネットワークの基本を確認する。  \n",
    "次のSprintで層を自由に変えられる設計にしていく。  \n",
    "\n",
    "**データセットの用意**  \n",
    "MNISTデータセットを使用する。Kerasによりデータセットをダウンロードし、展開まで行う。  \n",
    "\n",
    "**MNISTとは**  \n",
    "画像分類のための定番データセットで、手書き数字認識を行う。  \n",
    "このデータセットには学習用6万枚、テスト用1万枚の28×28ピクセルの白黒画像、およびそれらが0〜9のどの数字であるかが含まれている。\n",
    "\n",
    "**画像データとは**  \n",
    "一般的に白黒画像であればピクセルには0〜255の値が含まれる。  \n",
    "一方、カラー画像であればR（赤）、G（緑）、B（青）それぞれに対応する0〜255の値が含まれる。  \n",
    "機械学習をする上では、この0〜255の値一つひとつが特徴量として扱う。  \n",
    "0〜255は符号なしの8ビット整数で表せる範囲になるため、NumPyであれば「uint8」型の変数として保持できる。  \n",
    "\n",
    "**データセットの確認**  \n",
    "各データは28×28ピクセルの白黒画像となっている。  \n",
    "\n",
    "**平滑化**  \n",
    "(1, 28, 28)の各画像を、(1, 784)に変換する。  \n",
    "これまで学んできた機械学習手法や、今回扱う全結合層のみのニューラルネットワークではこの形で扱う。  \n",
    "全てのピクセルが一列になっていることを、平滑化（flatten）してあると表現する。\n",
    "\n",
    "**補足**  \n",
    "ここまで機械学習を学んでくる中で、特徴量の数を「次元」と呼んできた。  \n",
    "その視点ではMNISTは784次元のデータとなる。  \n",
    "一方で、NumPyのshapeが(784,)の状態を1次元配列とも呼ぶ。  \n",
    "画像としての縦横の情報を持つ（28, 28)の状態であれば、2次元配列となる。この視点では2次元のデータである。  \n",
    "さらに、もしもカラー画像であれば(28, 28, 3)ということになり、3次元配列となる。先ほどの視点では3次元のデータになる。  \n",
    "しかし、白黒でもカラーでも平面画像であり、立体データではないという視点で、2次元のデータである。  \n",
    "画像データを扱う際にはこのように「次元」という言葉が複数の意味合いで使われることに注意する。\n",
    "\n",
    "**画像データの可視化**  \n",
    "plt.imshowに渡す。  \n",
    "[matplotlib.pyplot.imshow — Matplotlib 3.0.2 documentation](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html \"matplotlib.pyplot.imshow — Matplotlib 3.0.2 documentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ生成\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像データの可視化\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# X_train[index]: (784,)\n",
    "# image: (28, 28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**発展的話題**  \n",
    "画像データは符号なし8ビット整数のuint8型で保持されることが一般的だが、plt.imshowはより自由な配列を画像として表示することが可能。例えば、以下のようにマイナスの値を持ったfloat64型の浮動小数点であってもエラーにはならず、先ほどと全く同じように表示される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "image = image.astype(np.float) # float型に変換\n",
    "image -= 105.35 # 意図的に負の小数値を作り出してみる\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()\n",
    "print(image) # 値を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは、自動的に値を0〜255の整数に変換して処理するように作られているからである。  \n",
    "uint8型であっても最小値が0、最大値が255でない場合には色合いがおかしくなる。  \n",
    "それを防ぐためには次のように引数を入れる。  \n",
    "画像関係のライブラリではこの自動的なスケーリングが思わぬ結果を生むことがあるので、新しいメソッドを使うときには確認しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image, 'gray', vmin = 0, vmax = 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**前処理**  \n",
    "画像は0から255のuint8型で表されるが、機械学習をする上では0から1のfloat型で扱うことになる。  \n",
    "色は理想的には連続値で、それを特徴量とするからであり、以下のコードで変換可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、正解ラベルは0から9の整数だが、ニューラルネットワークで多クラス分類を行う際にはone-hot表現に変換する。  \n",
    "scikit-learnのOneHotEncoderを使用したコードが以下の通り。  \n",
    "このone-hot表現による値はそのラベルである確率を示していることになるため、float型で扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape) # (60000,)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、学習用データ6万枚の内2割を検証用データとして分割する。学習用データが48000枚、検証用データが12000枚となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape) # (48000, 784)\n",
    "print(X_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ニューラルネットワーク分類器のクラスを作成\n",
    "ニューラルネットワーク分類器のクラスScratchSimpleNeuralNetrowkClassifierを作成する。  \n",
    "基本的な構成は機械学習編の線形回帰やロジスティック回帰などと同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose = True):\n",
    "        self.verbose = verbose\n",
    "        pass\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ミニバッチ処理**  \n",
    "これまでの機械学習スクラッチでは、全てのサンプルを一度に計算していた。  \n",
    "しかし、ニューラルネットワークではデータを分割して入力する確率的勾配降下法が一般的。  \n",
    "分割した際のひとかたまりをミニバッチ、そのサンプル数をバッチサイズと呼ぶ。\n",
    "\n",
    "今回はバッチサイズを10とする。今回使う学習用データは48000枚なので、4800回の更新を繰り返すことになる。  \n",
    "ニューラルネットワークではこれを4800回イテレーション（iteration）すると呼ぶ。  \n",
    "学習データを一度全て見ると1回のエポック（epoch）が終わったことになる。  \n",
    "このエポックを複数回繰り返し、学習が完了する。  \n",
    "\n",
    "イテレータは以下の通り。for文で呼び出すと、ミニバッチを取得できる。  \n",
    "\\_\\_getitem\\_\\_や\\_\\_next\\_\\_は\\_\\_init\\_\\_などと同じ特殊メソッドの一種。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクラスをニューラルネットワークのクラス内でインスタンス化し、for文を使うことでミニバッチが取り出せる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下をニューラルネットワークのクラス内で呼び出す\n",
    "\n",
    "get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=10)\n",
    "\n",
    "print(len(get_mini_batch)) # 4800\n",
    "print(get_mini_batch[5]) # 5番目のミニバッチが取得できる\n",
    "for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    # このfor文内でミニバッチが使える\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**フォワードプロパゲーション**  \n",
    "三層のニューラルネットワークのフォワードプロパゲーションを作成する。  \n",
    "以下の説明ではノード数は1層目は400、2層目は200とするが、変更しても構わない。\n",
    "\n",
    "各層の数式を以下に示す。今回はそれぞれの記号が表す配列が、実装上どのようなndarrayのshapeになるかを併記してある。\n",
    "\n",
    "batch_size = 10 # バッチサイズ  \n",
    "n_features = 784 # 特徴量の数  \n",
    "n_nodes1 = 400 # 1層目のノード数  \n",
    "n_nodes2 = 200 # 2層目のノード数  \n",
    "n_output = 10 # 出力のクラス数（3層目のノード数）  \n",
    "\n",
    "「1層目」\n",
    "$$A_1=X⋅W_1+B_1$$\n",
    "\n",
    "- $X$ : 特徴量ベクトル (batch_size, n_features)\n",
    "- $W_1$ : 1層目の重み (n_features, n_nodes1)\n",
    "- $B_1$ : 1層目のバイアス (n_nodes1,)\n",
    "- $A_1$ : 出力 (batch_size, n_nodes1)\n",
    "\n",
    "「1層目の活性化関数」\n",
    "$$Z_1=f(A_1)$$\n",
    "- $f()$ : 活性化関数\n",
    "- $Z_1$ : 出力 (batch_size, n_nodes1)\n",
    "\n",
    "「2層目」\n",
    "$$A_2=Z_1⋅W_2+B_2$$\n",
    "- $W_2$ : 2層目の重み (n_nodes1, n_nodes2)\n",
    "- $B_2$ : 2層目のバイアス (n_nodes2,)\n",
    "- $A_2$ : 出力 (batch_size, n_nodes2)\n",
    "\n",
    "「2層目の活性化関数」\n",
    "$$Z_2=f(A_2)$$\n",
    "- $f()$ : 活性化関数\n",
    "- $Z_2$ :  出力 (batch_size, n_nodes2)\n",
    "\n",
    "「3層目（出力層）」\n",
    "$$A_3=Z_2⋅W3+B_3$$\n",
    "- $W_3$ : 3層目の重み (n_nodes2, n_output)\n",
    "- $B_3$ : 3層目のバイアス (n_output,)\n",
    "- $A_3$ : 出力 (batch_size, n_output)\n",
    "\n",
    "「3層目の活性化関数」\n",
    "$$Z_3=softmax(A_3)$$\n",
    "- $softmax()$ : ソフトマックス関数\n",
    "- $Z_3$ : 出力 (batch_size, n_output)\n",
    "\n",
    "$Z_3$は各ラベル（0〜9）に対する確率の配列である。\n",
    "\n",
    "**重みの初期値**  \n",
    "ニューラルネットワークにおいては重みの初期値は重要な要素となる。  \n",
    "様々な方法が提案されているが、今回はガウス分布による単純な初期化を行う。バイアスに関しても同様。\n",
    "\n",
    "標準偏差の値$sigma$はハイパーパラメータ。発展的な重みの初期化方法については次のSprintで扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 784\n",
    "n_nodes1 = 400\n",
    "sigma = 0.01 # ガウス分布の標準偏差\n",
    "W1 = sigma * np.random.randn(n_features, n_nodes1)\n",
    "# W1: (784, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**活性化関数（フォワードプロバゲーション）**  \n",
    "活性化関数を作成し、フォワードプロパゲーションの中で使用する。  \n",
    "切り替えられるように実装することを推奨するが、片方でも構わない。\n",
    "\n",
    "「シグモイド関数」  \n",
    "$$f(Z) = sigmoid(A) = \\frac{1}{1+exp(-A)}$$\n",
    "指数関数$exp(−A)$の計算はnp.expを使用する。\n",
    "\n",
    "「ハイパボリックタンジェント関数」  \n",
    "次の数式で表されるが、np.tanhひとつで実現できる。\n",
    "$$f(Z) = tanh(A) = \\frac{exp(A) - exp(-A)}{exp(A) + exp(-A)}$$\n",
    "＊現在ではこれらの代わりにReLUと呼ばれる活性化関数が一般的。次のSprintで扱う。  \n",
    "\n",
    "**ソフトマックス関数**  \n",
    "ソフトマックス関数を作成し、フォワードプロパゲーションの中で使用する。  \n",
    "これも活性化関数の一種だが、多クラス分類の出力層で使われる特性上、区別して扱われることが多い。\n",
    "$$Z_{3\\_k} = \\frac{exp(A_{3\\_k})}{\\sum_{i=1}^{n}exp(A_{3\\_i})}$$\n",
    "\n",
    "- $Z_{3_k}$ : $k$番目のクラスの確率ベクトル (batch_size,)\n",
    "- $A_{3_k}$ : $k$番目のクラスにあたる前の層からのベクトル (batch_size,)\n",
    "- $n$ : クラスの数、n_output。今回のMNISTでは10。\n",
    "\n",
    "分母は全てのクラスに相当する値を指数関数に通した上で足し合わせたもの。  \n",
    "その中で、分子に$k$番目のクラスを持ってくることで、$k$番目のクラスである確率が求まる。  \n",
    "これを10クラス分計算し、合わせたものが$Z_3$。  \n",
    "\n",
    "**交差エントロピー誤差**  \n",
    "目的関数（損失関数）を作成する。  \n",
    "多クラス分類の目的関数である交差エントロピー誤差$L$は次の数式。\n",
    "$$L = - \\sum_{i}^{n}Y_i log(Z_{3\\_i})$$\n",
    "$Y_i$ : $i$番目のクラスの正解ラベル（one-hot表現で0か1）\n",
    "\n",
    "**バックプロパゲーション**  \n",
    "三層のニューラルネットワークのバックプロパゲーションを作成する。確率的勾配降下法を行う部分。\n",
    "\n",
    "まず、$i$層目の重みとバイアスの更新式である。  \n",
    "$W_i$と$B_i$に対し、更新後の$W_i^{\\prime}$と$B_i^{\\prime}$は次の数式で求められる。\n",
    "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i})$$\n",
    "$$B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
    "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）  \n",
    "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$に関する損失$L$の勾配  \n",
    "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$に関する損失$L$の勾配  \n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "この更新方法はSprint3線形回帰やsprint4ロジスティック回帰における最急降下法と同様。  \n",
    "より効果的な更新方法が知られており、それは次のSprintで扱う。\n",
    "勾配$\\frac{\\partial L}{\\partial W_i}$や$\\frac{\\partial L}{\\partial B_i}$を求めるために、バックプロパゲーションを行う。  \n",
    "ハイパボリックタンジェント関数を使用した例を載せた。シグモイド関数の場合の数式はその後ろにある。\n",
    "\n",
    "「3層目」  \n",
    "$$\\frac{\\partial L}{\\partial A_3} = Z_3 - Y$$\n",
    "$$\\frac{\\partial L}{\\partial B_3} = \\frac{\\partial L}{\\partial A_3}$$\n",
    "$$\\frac{\\partial L}{\\partial W_3} = Z_2^T \\cdot \\frac{\\partial L}{\\partial A_3}$$\n",
    "$$\\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial A_3} \\cdot W_3^T$$\n",
    "$\\frac{\\partial L}{\\partial A_3}$ : $A_3$に関する損失$L$の勾配 (batch_size, n_output)  \n",
    "$\\frac{\\partial L}{\\partial B_3}$ : $B_3$に関する損失$L$の勾配 (batch_size, n_output)  \n",
    "$\\frac{\\partial L}{\\partial W_3}$ : $W_3$に関する損失$L$の勾配 (n_nodes2, n_output)  \n",
    "$\\frac{\\partial L}{\\partial Z_2}$ : $Z_2$に関する損失$L$の勾配 (batch_size, n_nodes2)  \n",
    "$Z_3$ : フォワードプロパゲーションの出力 (batch_size, n_output)  \n",
    "$Y$ : 正解ラベルのベクトル (batch_size, n_output)  \n",
    "$Z^T_2$ : 転置した2層目の出力 (n_nodes2, batch_size)  \n",
    "$W^T_3$ : 転置した3層目の重み (n_output, n_nodes2)\n",
    "\n",
    "「2層目」  \n",
    "$$\\frac{\\partial L}{\\partial A_2} = \\frac{\\partial L}{\\partial Z_2} × \\{1-tanh^2(A_2)\\}$$\n",
    "$$\\frac{\\partial L}{\\partial B_2} = \\frac{\\partial L}{\\partial A_2}$$\n",
    "$$\\frac{\\partial L}{\\partial W_2} = Z_1^T \\cdot \\frac{\\partial L}{\\partial A_2}$$\n",
    "$$\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_2} \\cdot W_2^T$$\n",
    "$\\frac{\\partial L}{\\partial A_2}$ : $A_2$に関する損失$L$の勾配 (batch_size, n_nodes2)  \n",
    "$\\frac{\\partial L}{\\partial B_2}$ : $B_2$に関する損失$L$の勾配 (batch_size, n_nodes2)  \n",
    "$\\frac{\\partial L}{\\partial W_2}$ : $W_2$に関する損失$L$の勾配 (n_nodes1, n_nodes2)  \n",
    "$\\frac{\\partial L}{\\partial Z_1}$ : $Z_1$に関する損失$L$の勾配 (batch_size, n_nodes1)\n",
    "\n",
    "「1層目」  \n",
    "$$\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_1} × \\{1-tanh^2(A_1)\\}$$\n",
    "$$\\frac{\\partial L}{\\partial B_1} = \\frac{\\partial L}{\\partial A_1}$$\n",
    "$$\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\frac{\\partial L}{\\partial A_1}$$\n",
    "$\\frac{\\partial L}{\\partial A_1}$ : $A_1$に関する損失$L$の勾配 (batch_size, n_nodes1)  \n",
    "$\\frac{\\partial L}{\\partial B_1}$ : $B_1$に関する損失$L$の勾配 (batch_size, n_nodes1)  \n",
    "$\\frac{\\partial L}{\\partial W_1}$ : $W_1$に関する損失$L$の勾配 (n_features, n_nodes1)  \n",
    "$A_1$ : フォワードプロパゲーションの1層目の出力 (batch_size, n_nodes1)  \n",
    "$X^T$ : 転置した特徴量ベクトル (n_feature, batch_size)  \n",
    "$W^T_1$ : 転置した1層目の重み (n_nodes1, n_features)  \n",
    "\n",
    "**補足**  \n",
    "活性化関数にシグモイド関数を使用した場合は、次のようになる。\n",
    "$$\\frac{\\partial L}{\\partial A_2} = \\frac{\\partial L}{\\partial Z_2} ×  \\{1-sigmoid(A_2)\\}sigmoid(A_2)$$\n",
    "$$\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial Z_1} ×  \\{1-sigmoid(A_1)\\}sigmoid(A_1)$$\n",
    "\n",
    "**推定**  \n",
    "推定を行うメソッドを作成する。  \n",
    "フォワードプロパゲーションによって出力された10個の確率の中で、最も高いものはどれかを判定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 学習曲線のプロット\n",
    "学習曲線をプロットする。  \n",
    "ニューラルネットワークは過学習が発生しやすいため、学習曲線の確認が重要。  \n",
    "trainデータとvalデータに対するエポックごとの損失（交差エントロピー誤差）を記録できるようにする必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 指標値の算出\n",
    "分類に関する指標値で精度を確認する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（オプション）誤分類の確認**  \n",
    "誤分類した画像はどのようなものだったかを見てみる。推定値を用意し、以下のコードを実行する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "語分類結果を並べて表示する。画像の上の表示は「推定結果/正解」である。\n",
    "\n",
    "Parameters:\n",
    "----------\n",
    "y_pred : 推定値のndarray (n_samples,)\n",
    "y_val : 検証用データの正解ラベル(n_samples,)\n",
    "X_val : 検証用データの特徴量（n_samples, n_features)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num = 36 # いくつ表示するか\n",
    "\n",
    "true_false = y_pred==y_val\n",
    "false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "\n",
    "if false_list.shape[0] < num:\n",
    "    num = false_list.shape[0]\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "for i in range(num):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "    ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
