{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term2 Sprint18 授業課題 \n",
    "## コーディング課題：公開されている実装を動かす"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprint16で読んだFaster R-CNN[1]の実装を動かす。\n",
    "\n",
    "[1]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n",
    "以下のものを使用する(Kerasを使用した実装です)。\n",
    "\n",
    "[duckrabbits/ObjectDetection at master](https://github.com/duckrabbits/ObjectDetection/tree/master \"duckrabbits/ObjectDetection at master\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 学習と推定\n",
    "READMEを参考に上記実装を動かす。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 論文と実装の対応\n",
    "コードリーディングを行う。\n",
    "\n",
    "まず、Faster R-CNN[1]において重要だと考えた部分を列挙する。そして、それに対応するコードを見つける。\n",
    "\n",
    "（例）\n",
    "\n",
    "- RPNを実現しているコードはどこか\n",
    "- RoIプーリングを実現しているコードはどこか\n",
    "\n",
    "フレームワークには畳み込み層など一般的なものはクラスが用意されているが、RoIプーリングなど特定の手法限定のものは用意されていない。  \n",
    "オリジナルのレイヤーを作成することが可能であり、Kerasであれば以下のページに情報がまとまっている。\n",
    "\n",
    "[オリジナルのKerasレイヤーを作成する - Keras Documentation](https://keras.io/ja/layers/writing-your-own-keras-layers/ \"オリジナルのKerasレイヤーを作成する - Keras Documentation\")\n",
    "\n",
    "**参考**\n",
    "\n",
    "KerasではVGG16のクラスが用意されているため、簡単に利用ができる。  \n",
    "include_top=Falseの引数を与えることで、出力のための全結合層部分が除かれる。  \n",
    "weights='imagenet'でImageNetを利用した学習済みモデルも手に入り、転移学習が行える。  \n",
    "weights='None'とすればランダムな初期化となる。\n",
    "\n",
    "[Applications - Keras Documentation](https://keras.io/ja/applications/#vgg16 \"Applications - Keras Documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習済みの重みによる推定\n",
    "\n",
    "シンプソンズのデータセットをFaster R-CNN以外の手法で学習・推定を行う。YOLOv3[2]のKeras実装を使う。\n",
    "\n",
    "[qqwweee/keras-yolo3: A Keras implementation of YOLOv3 (Tensorflow backend)](https://github.com/qqwweee/keras-yolo3 \"qqwweee/keras-yolo3: A Keras implementation of YOLOv3 (Tensorflow backend)\")\n",
    "\n",
    "[2]Jeseph Redmon, Ali Farhadi. YOLOv3: An Incremental Improvement\n",
    "\n",
    "https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "\n",
    "**Sprint18で使用した実装（再掲）**\n",
    "\n",
    "[lasershow/SimpsonRecognition: Detect and recognize The Simpsons characters using Keras and Faster R-CNN](https://github.com/lasershow/SimpsonRecognition \"lasershow/SimpsonRecognition: Detect and recognize The Simpsons characters using Keras and Faster R-CNN\")\n",
    "\n",
    "学習済みの重みを使い推定を行う方法がREADME.mdのQuick Startに記載されている。\n",
    "\n",
    "まずはこの通りにして各自何かしらの画像や動画に対して検出を行う。\n",
    "\n",
    "出力結果を課題の一部として提出する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習のためのファイルを作成\n",
    "新しいデータ（シンプソンズデータセット）を学習する。  \n",
    "README.mdのTrainingを読み、シンプソンズデータセットを学習するために必要なファイルを作成する。\n",
    "\n",
    "アノテーションファイルの形式がSprint18で扱った実装のものとは異なっているので、変換する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習\n",
    "問題2で作成したファイルを使用して学習する。  \n",
    "実行環境で学習に時間がかかる場合は、学習が行えることを確認するのみで終えて構わない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 論文と実装の対応（アドバンス課題）\n",
    "コードリーディングを行う。\n",
    "\n",
    "まず、YOLOv3[2]の論文において重要だと考えた部分を列挙する。そして、それに対応するコードを見つける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
