{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term1 Sprint1 授業前課題 \n",
    "## コーディング課題：train_test_split, 分類・回帰パイプラインのスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. train_test_splitのスクラッチ\n",
    "スクラッチの練習として、scikit-learnのtrain_test_splitを自作してみる。  \n",
    "Jupyter Notebookでコーディングを進め、完成後はpyファイルとする。  \n",
    "utilsディレクトリの中にsplit.pyを作る。  \n",
    "scikit-learnのtrain_test_splitと同じ動作をしているか必ずテストをする。  \n",
    "[sklearn.model_selection.train_test_split — scikit-learn 0.20.0 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html \"sklearn.model_selection.train_test_split — scikit-learn 0.20.0 documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下をutilsディレクトリにsplit.pyとして作成済\n",
    "def train_test_split(X,y,test_size=0.25,\n",
    "                     random_state=None,shuffle=True,stratify=None):\n",
    "    \"\"\"\n",
    "    Split the data to be learned and tested.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "      data to be learned and tested\n",
    "    y : ndarray, shape (n_samples, )\n",
    "      objective labels\n",
    "    test_size : float (0<test_size<1)(default: 0.25)\n",
    "      set the rate of test size\n",
    "    random_state : int\n",
    "      set the pseudo-random number to be used in RandomStateGenerator\n",
    "    shuffle : boolean (default:True)\n",
    "      shuffle before split or not. If False, set stratify as None.\n",
    "    stratify : array-like or None\n",
    "      array for stratified sampling\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train : ndarray, shape (n_samples, n_features)\n",
    "      data to be learned\n",
    "    X_test : ndarray, shape (n_samples, n_features)\n",
    "      data to be tested\n",
    "    y_train : ndarray, shape (n_samples, )\n",
    "      labels for X_train\n",
    "    y_test : ndarray, shape (n_samples, )\n",
    "      labels for X_test\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # Error if feature samples number does not corresponds to y number.\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X samples number({}) is not same as y {}.\".format(\n",
    "                X.shape[0], y.shape[0]))\n",
    "    \n",
    "    # make several parameters to be used\n",
    "    n_samples = X.shape[0]\n",
    "    n_train = np.floor((1-test_size) * n_samples).astype(int)\n",
    "    n_test = n_samples - n_train\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    class_counts = np.bincount(y)\n",
    "    class_indices = np.split(np.argsort(y, kind='mergesort'),\n",
    "                             np.cumsum(class_counts)[:-1])\n",
    "    \n",
    "    # Case1: Shuffle=False and stratify=None\n",
    "    if shuffle is False and stratify is None:\n",
    "        X_test = X[:n_test]\n",
    "        X_train = X[n_test:(n_test + n_train)]\n",
    "        y_test = y[:n_test]\n",
    "        y_train = y[n_test:(n_test + n_train)]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # Case2: Shuffle=False and stratify=y\n",
    "    elif shuffle is False and stratify is not None:\n",
    "        raise ValueError(\"If 'shuffle' parameter is False, \"\n",
    "                         \"then 'stratify' parameter should be None.\")\n",
    "    \n",
    "    # Case3: Shuffle=True and stratify=None\n",
    "    elif shuffle is True and stratify is None:\n",
    "        rng = np.random.RandomState(seed=random_state)\n",
    "        # shuffle and split\n",
    "        permutation = rng.permutation(n_samples)\n",
    "        ind_test = permutation[:n_test]\n",
    "        ind_train = permutation[n_test:(n_test + n_train)]\n",
    "        \n",
    "        X_train = X[ind_train]\n",
    "        X_test = X[ind_test]\n",
    "        y_train = y[ind_train]\n",
    "        y_test = y[ind_test]\n",
    "        \n",
    "        yield X_train\n",
    "        yield X_test\n",
    "        yield y_train\n",
    "        yield y_test\n",
    "    \n",
    "    # Case4: Shuffle=True and stratify=y\n",
    "    else:\n",
    "        def extracting_func(class_counts, n_draws, rng):\n",
    "            \"\"\"\n",
    "            Stratified sampling at random a certain number(n_draws) of samples \n",
    "            from population in class_counts.\n",
    "            \n",
    "            \"\"\"\n",
    "            # assign each number of samples to be extracted per each class\n",
    "            continuous = n_draws * (class_counts / class_counts.sum())\n",
    "            floored = np.floor(continuous)\n",
    "            need_to_add = int(n_draws - floored.sum())\n",
    "            # determine which classes should be added one more because of flooring\n",
    "            if need_to_add > 0:\n",
    "                remainder = continuous - floored\n",
    "                # sort the remaining values in an unascending manner\n",
    "                values = np.sort(np.unique(remainder))[::-1]\n",
    "                for value in values:\n",
    "                    inds, = np.where(remainder == value)\n",
    "                    # set the number of value to be added\n",
    "                    add_now = min(len(inds), need_to_add)\n",
    "                    # determine at random where should be added\n",
    "                    inds = rng.choice(inds, size=add_now, replace=False)\n",
    "                    floored[inds] += 1\n",
    "                    # repeat until when 'need to add' becomes 0\n",
    "                    need_to_add -= add_now\n",
    "                    if need_to_add == 0:\n",
    "                        break\n",
    "            return floored.astype(np.int)\n",
    "        \n",
    "        # set a number of samples to be selected per each class\n",
    "        rng = np.random.RandomState(seed=random_state)\n",
    "        n_i = extracting_func(class_counts, n_train, rng)\n",
    "        class_counts_remaining = class_counts - n_i\n",
    "        t_i = extracting_func(class_counts_remaining, n_test, rng)\n",
    "        \n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        # select at random which indices should be assigned to train and test set\n",
    "        for i in range(n_classes):\n",
    "            permutation = rng.permutation(class_counts[i])\n",
    "            perm_indices_class_i = class_indices[i].take(\n",
    "                    permutation,mode='clip')\n",
    "            train.extend(perm_indices_class_i[:n_i[i]])\n",
    "            test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])\n",
    "        \n",
    "        ind_train = rng.permutation(train)\n",
    "        ind_test = rng.permutation(test)\n",
    "        \n",
    "        X_train = X[ind_train]\n",
    "        X_test = X[ind_test]\n",
    "        y_train = y[ind_train]\n",
    "        y_test = y[ind_test]\n",
    "        \n",
    "        yield X_train\n",
    "        yield X_test\n",
    "        yield y_train\n",
    "        yield y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------test_size=0.25, stratify=None------\n",
      "X_trainの各要素との差の絶対値の合計： 0.0\n",
      "X_testの各要素との差の絶対値の合計： 0.0\n",
      "y_trainの各要素との差の絶対値の合計： 0\n",
      "y_testの各要素との差の絶対値の合計： 0\n",
      "------test_size: 0.02~0.50, stratify=None------\n",
      "全てのtest_sizeにおける、全変数の各要素との差の絶対値の合計： 0.0\n",
      "------test_size: 0.02~0.50, stratify=y------\n",
      "全てのtest_sizeにおける、全変数の各要素との差の絶対値の合計： 0.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# scratchのtrain_test_splitのimport\n",
    "sys.path.append(\"../ml-scratch/utils/\")\n",
    "import split\n",
    "from importlib import reload\n",
    "reload(split)\n",
    "\n",
    "# sklearnのtrain_test_splitのimport\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = split.train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "X_train_sklearn, X_test_sklearn, y_train_sklearn, y_test_sklearn = sk_train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# 以下が全て0であれば実装成功\n",
    "print(\"------test_size=0.25, stratify=None------\")\n",
    "print(\"X_trainの各要素との差の絶対値の合計：\", np.abs(X_train_scratch - X_train_sklearn).sum())\n",
    "print(\"X_testの各要素との差の絶対値の合計：\", np.abs(X_test_scratch - X_test_sklearn).sum())\n",
    "print(\"y_trainの各要素との差の絶対値の合計：\", np.abs(y_train_scratch - y_train_sklearn).sum())\n",
    "print(\"y_testの各要素との差の絶対値の合計：\", np.abs(y_test_scratch - y_test_sklearn).sum())\n",
    "\n",
    "\n",
    "# test_sizeを0.02から0.50までで試行（test_size=0.01はクラス数以下となるので設定不可）\n",
    "print(\"------test_size: 0.02~0.50, stratify=None------\")\n",
    "results = []\n",
    "for i in np.arange(0.02, 0.51, 0.01):\n",
    "    X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = split.train_test_split(\n",
    "        X, y, test_size=i, random_state=0)\n",
    "    X_train_sklearn, X_test_sklearn, y_train_sklearn, y_test_sklearn = sk_train_test_split(\n",
    "        X, y, test_size=i, random_state=0)\n",
    "    results.append(np.abs(X_train_scratch - X_train_sklearn).sum())\n",
    "    results.append(np.abs(X_test_scratch - X_test_sklearn).sum())\n",
    "    results.append(np.abs(y_train_scratch - y_train_sklearn).sum())\n",
    "    results.append(np.abs(y_test_scratch - y_test_sklearn).sum())\n",
    "print(\"全てのtest_sizeにおける、全変数の各要素との差の絶対値の合計：\", sum(results))\n",
    "\n",
    "\n",
    "# stratify=yで同様に試行\n",
    "print(\"------test_size: 0.02~0.50, stratify=y------\")\n",
    "results = []\n",
    "for i in np.arange(0.02, 0.51, 0.01):\n",
    "    X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = split.train_test_split(\n",
    "        X, y, test_size=i, random_state=0, stratify=y)\n",
    "    X_train_sklearn, X_test_sklearn, y_train_sklearn, y_test_sklearn = sk_train_test_split(\n",
    "        X, y, test_size=i, random_state=0, stratify=y)\n",
    "    results.append(np.abs(X_train_scratch - X_train_sklearn).sum())\n",
    "    results.append(np.abs(X_test_scratch - X_test_sklearn).sum())\n",
    "    results.append(np.abs(y_train_scratch - y_train_sklearn).sum())\n",
    "    results.append(np.abs(y_test_scratch - y_test_sklearn).sum())\n",
    "print(\"全てのtest_sizeにおける、全変数の各要素との差の絶対値の合計：\", sum(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分類パイプラインの作成\n",
    "分類は3種類の手法を扱う。pyファイルで実行できる分類のパイプラインを作成する。\n",
    "- ロジスティック回帰\n",
    "- SVM\n",
    "- 決定木\n",
    "\n",
    "データセットは3種類用意する。\n",
    "\n",
    "1つ目は事前学習期間同様にirisデータセットとする。  \n",
    "[sklearn.datasets.load_iris — scikit-learn 0.20.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html \"sklearn.datasets.load_iris — scikit-learn 0.20.2 documentation\")\n",
    "\n",
    "2値分類としたいため、以下の2つの目的変数のみ利用し、特徴量は4種類全て使う。\n",
    "- versicolor\n",
    "- virginica\n",
    "\n",
    "また、残り2つは可視化が可能な特徴量が2つのデータセットを人工的に用意する。続くコードで説明変数X,目的変数yが作成可能。「シンプルデータセット1」、「シンプルデータセット2」とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下をmodelsディレクトリにpipe_clf.pyとして作成済\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def log_reg(X_train, X_test, y_train, y_test):\n",
    "    estimators = [('sc', StandardScaler()),\n",
    "                  ('lr', LogisticRegression())]\n",
    "    parameters = {\"lr__penalty\" : [\"l1\",\"l2\"],\n",
    "                  \"lr__C\" : np.logspace(-3, 3, 7).tolist(), \n",
    "                  \"lr__solver\" : [\"liblinear\"]}\n",
    "    pl = Pipeline(estimators)\n",
    "    clf = GridSearchCV(pl, parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(\"------Logistic Regression------\")\n",
    "    print(\"訓練データの正解率: \", accuracy_score(y_train, y_train_pred))\n",
    "    print(\"テストデータの正解率: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの適合率: \", precision_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの再現率: \", recall_score(y_test, y_test_pred))\n",
    "    print(\"テストデータのf1スコア: \", f1_score(y_test, y_test_pred))\n",
    "    \n",
    "def svc(X_train, X_test, y_train, y_test):\n",
    "    estimators = [('sc', StandardScaler()),\n",
    "                  ('svc', SVC())]\n",
    "    parameters = {\"svc__C\" : np.logspace(-3, 3, 7).tolist()}\n",
    "    pl = Pipeline(estimators)\n",
    "    clf = GridSearchCV(pl, parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(\"------SVC------\")\n",
    "    print(\"訓練データの正解率: \", accuracy_score(y_train, y_train_pred))\n",
    "    print(\"テストデータの正解率: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの適合率: \", precision_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの再現率: \", recall_score(y_test, y_test_pred))\n",
    "    print(\"テストデータのf1スコア: \", f1_score(y_test, y_test_pred))\n",
    "\n",
    "def dec_tree(X_train, X_test, y_train, y_test):\n",
    "    estimators = [('sc', StandardScaler()),\n",
    "                  ('dtc', DecisionTreeClassifier())]\n",
    "    parameters = {\"dtc__max_depth\" : np.arange(1,11).tolist()}\n",
    "    pl = Pipeline(estimators)\n",
    "    clf = GridSearchCV(pl, parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(\"------Decision Tree Classifier------\")\n",
    "    print(\"訓練データの正解率: \", accuracy_score(y_train, y_train_pred))\n",
    "    print(\"テストデータの正解率: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの適合率: \", precision_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの再現率: \", recall_score(y_test, y_test_pred))\n",
    "    print(\"テストデータのf1スコア: \", f1_score(y_test, y_test_pred))\n",
    "\n",
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    estimators = [('sc', StandardScaler()),\n",
    "                  ('rfc', RandomForestClassifier())]\n",
    "    parameters = {\"rfc__max_depth\" : np.arange(1,11).tolist(), \n",
    "                  \"rfc__n_estimators\" : np.arange(1,21).tolist()}\n",
    "    pl = Pipeline(estimators)\n",
    "    clf = GridSearchCV(pl, parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(\"------Random Forest Classifier------\")\n",
    "    print(\"訓練データの正解率: \", accuracy_score(y_train, y_train_pred))\n",
    "    print(\"テストデータの正解率: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの適合率: \", precision_score(y_test, y_test_pred))\n",
    "    print(\"テストデータの再現率: \", recall_score(y_test, y_test_pred))\n",
    "    print(\"テストデータのf1スコア: \", f1_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irisデータセット\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data[50:] # versicolorとverginicaのみに該当する特徴量\n",
    "y = iris.target[50:] # versicolorとverginica\n",
    "y = np.where(y==1,0,1) # ラベルを0と1にリセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Logistic Regression------\n",
      "訓練データの正解率:  0.9866666666666667\n",
      "テストデータの正解率:  0.96\n",
      "テストデータの適合率:  0.9230769230769231\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  0.9600000000000001\n",
      "------SVC------\n",
      "訓練データの正解率:  0.9866666666666667\n",
      "テストデータの正解率:  0.92\n",
      "テストデータの適合率:  0.8571428571428571\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  0.923076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Decision Tree Classifier------\n",
      "訓練データの正解率:  0.9733333333333334\n",
      "テストデータの正解率:  0.84\n",
      "テストデータの適合率:  0.75\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  0.8571428571428571\n",
      "------Random Forest Classifier------\n",
      "訓練データの正解率:  0.9733333333333334\n",
      "テストデータの正解率:  0.88\n",
      "テストデータの適合率:  0.8\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  0.888888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 実際に実行\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# scratchの分類器パイプラインファイルをimport\n",
    "sys.path.append(\"../ml-scratch/models/\")\n",
    "import pipe_clf\n",
    "reload(pipe_clf)\n",
    "\n",
    "X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = sk_train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pipe_clf.log_reg(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.svc(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.dec_tree(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.random_forest(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルデータセット1\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "n_samples = 500\n",
    "f0 = [-1, 2]\n",
    "f1 = [2, -1]\n",
    "cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "\n",
    "f0 = np.random.multivariate_normal(f0, cov, int(n_samples/2))\n",
    "f1 = np.random.multivariate_normal(f1, cov, int(n_samples/2))\n",
    "\n",
    "X = np.concatenate((f0, f1))\n",
    "y = np.concatenate((np.ones((int(n_samples/2))), np.ones((int(n_samples/2))) *(-1))).astype(np.int)\n",
    "\n",
    "random_index = np.random.permutation(np.arange(n_samples))\n",
    "X = X[random_index]\n",
    "y = y[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Logistic Regression------\n",
      "訓練データの正解率:  1.0\n",
      "テストデータの正解率:  1.0\n",
      "テストデータの適合率:  1.0\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  1.0\n",
      "------SVC------\n",
      "訓練データの正解率:  1.0\n",
      "テストデータの正解率:  1.0\n",
      "テストデータの適合率:  1.0\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Decision Tree Classifier------\n",
      "訓練データの正解率:  1.0\n",
      "テストデータの正解率:  0.992\n",
      "テストデータの適合率:  0.9866666666666667\n",
      "テストデータの再現率:  1.0\n",
      "テストデータのf1スコア:  0.9932885906040269\n",
      "------Random Forest Classifier------\n",
      "訓練データの正解率:  0.9813333333333333\n",
      "テストデータの正解率:  0.984\n",
      "テストデータの適合率:  1.0\n",
      "テストデータの再現率:  0.972972972972973\n",
      "テストデータのf1スコア:  0.9863013698630138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = sk_train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pipe_clf.log_reg(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.svc(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.dec_tree(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.random_forest(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルデータセット2\n",
    "X = np.array([[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "       [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "       [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "       [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "       [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "       [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "       [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "       [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "       [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "       [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "       [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "       [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "       [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "       [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "       [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "       [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "       [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "       [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "       [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "       [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Logistic Regression------\n",
      "訓練データの正解率:  0.6333333333333333\n",
      "テストデータの正解率:  0.3\n",
      "テストデータの適合率:  0.3333333333333333\n",
      "テストデータの再現率:  0.4\n",
      "テストデータのf1スコア:  0.3636363636363636\n",
      "------SVC------\n",
      "訓練データの正解率:  0.7333333333333333\n",
      "テストデータの正解率:  0.5\n",
      "テストデータの適合率:  0.5\n",
      "テストデータの再現率:  0.4\n",
      "テストデータのf1スコア:  0.4444444444444445\n",
      "------Decision Tree Classifier------\n",
      "訓練データの正解率:  1.0\n",
      "テストデータの正解率:  0.7\n",
      "テストデータの適合率:  0.75\n",
      "テストデータの再現率:  0.6\n",
      "テストデータのf1スコア:  0.6666666666666665\n",
      "------Random Forest Classifier------\n",
      "訓練データの正解率:  0.9666666666666667\n",
      "テストデータの正解率:  0.7\n",
      "テストデータの適合率:  0.75\n",
      "テストデータの再現率:  0.6\n",
      "テストデータのf1スコア:  0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = sk_train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pipe_clf.log_reg(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.svc(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.dec_tree(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)\n",
    "pipe_clf.random_forest(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 回帰パイプラインの作成\n",
    "回帰は1種類を扱う。pyファイルで実行できる回帰のパイプラインを作成する。\n",
    "- 線形回帰\n",
    "\n",
    "データセットは事前学習期間同様にHouse Pricesコンペティションのものを使う。  \n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data \"House Prices: Advanced Regression Techniques\")\n",
    "\n",
    "train.csvをダウンロードし、目的変数としてSalePrice、説明変数としてGrLivAreaとYearBuiltを使う。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下をmodelsディレクトリにpipe_reg.pyとして作成済\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def lin_reg(X_train, X_test, y_train, y_test):\n",
    "    estimators = [('sc', StandardScaler()),\n",
    "                  ('lr', LinearRegression())]\n",
    "    parameters = {\"lr__fit_intercept\" : [False, True]}\n",
    "    pl = Pipeline(estimators)\n",
    "    reg = GridSearchCV(pl, parameters, n_jobs=-1, cv=5)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_test_pred = reg.predict(X_test)\n",
    "    print(\"------Linear Regression------\")\n",
    "    print(\"テストデータのMSE: \", mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"テストデータのR2_score: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "X = train[[\"GrLivArea\",\"YearBuilt\"]].values\n",
    "y = train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Linear Regression------\n",
      "テストデータのMSE:  2725908099.0636373\n",
      "テストデータのR2_score:  0.5871035359601553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/arimoto/anaconda3/envs/python37/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# 実際に実行\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# scratchの回帰パイプラインファイルをimport\n",
    "sys.path.append(\"../ml-scratch/models/\")\n",
    "import pipe_reg\n",
    "reload(pipe_reg)\n",
    "\n",
    "X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch = sk_train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "pipe_reg.lin_reg(X_train_scratch, X_test_scratch, y_train_scratch, y_test_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
